{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce Paradigm\n",
    "\n",
    "Lacking a database, a common way to build a dataset is with the Map-Filter-Reduce paradigm.\n",
    "\n",
    "- Map transforms the database\n",
    "- Filter is a where clause to subselect the data\n",
    "- Reduce is effectively a GroupBy operation, folding data into aggregates.\n",
    "\n",
    "Let's look on an applied task:\n",
    "\n",
    "Given a list of strings, return the longest string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "python\nCPU times: user 457 µs, sys: 198 µs, total: 655 µs\nWall time: 475 µs\n"
    }
   ],
   "source": [
    "def find_longest_string(list_of_strings):\n",
    "    longest_string = None\n",
    "    longest_string_len = 0     \n",
    "    for s in list_of_strings:\n",
    "        if len(s) > longest_string_len:\n",
    "            longest_string_len = len(s)\n",
    "            longest_string = s\n",
    "    return longest_string\n",
    "\n",
    "list_of_strings = ['abc', 'python', 'dima'] * 1000\n",
    "%time max_length = print(find_longest_string(list_of_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "break our code into two steps: 1) compute the len of all strings and 2) select the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('python', 6)\n"
    }
   ],
   "source": [
    "# step 1:\n",
    "list_of_string_lens = [len(s) for s in list_of_strings]\n",
    "list_of_string_lens = zip(list_of_strings, list_of_string_lens)\n",
    "#step 2:\n",
    "max_len = max(list_of_string_lens, key=lambda t: t[1])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call “step one” a “mapper” because it maps some value into some other value\n",
    "\n",
    "call “step two” a reducer because it gets a list of values and produces a single (in most cases) value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = len\n",
    "\n",
    "def reducer(p, c):\n",
    "    \"\"\"\n",
    "    Takes two tuples as input \n",
    "    returns the one with the biggest len\n",
    "    \"\"\"\n",
    "    if p[1] > c[1]:\n",
    "        return p\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 maps our list of strings into a list of tuples using the mapper function (here I use the zip again to avoid duplicating the strings).\n",
    "\n",
    "Step 2 uses the reducer function, goes over the tuples from step one and applies it one by one. The result is a tuple with the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('python', 6)\n"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "#step 1\n",
    "mapped = map(mapper, list_of_strings)\n",
    "mapped = zip(list_of_strings, mapped)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_chunks = np.split(np.array(list_of_strings), 30)\n",
    "#step 1:\n",
    "reduced_all = []\n",
    "for chunk in data_chunks:\n",
    "    mapped_chunk = map(mapper, chunk)\n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    reduced_chunk = reduce(reducer, mapped_chunk)\n",
    "    reduced_all.append(reduced_chunk)\n",
    "    \n",
    "#step 2:\n",
    "reduced = reduce(reducer, reduced_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step one, we go over our chunks and find the longest string in that chunk using a map and reduce. \n",
    "\n",
    "In step two, we take the output of step one, which is a list of reduced values, and perform a final reduce to get the longest string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\nWall time: 12.2 µs\n('python', 6)\n"
    }
   ],
   "source": [
    "def chunks_mapper(chunk):\n",
    "    mapped_chunk = map(mapper, chunk) \n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    return reduce(reducer, mapped_chunk)\n",
    "    \n",
    "%time\n",
    "\n",
    "data_chunks = np.split(np.array(list_of_strings), 30)\n",
    "#step 1:\n",
    "mapped = map(chunks_mapper, data_chunks)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('python', 6)\n"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "N_THREADS = 7\n",
    "\n",
    "pool = Pool(N_THREADS)\n",
    "data_chunks = np.array_split(np.array(list_of_strings), N_THREADS)\n",
    "#step 1:\n",
    "mapped = pool.map(chunks_mapper, data_chunks)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('python', 6)\n"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "N_THREADS = 7\n",
    "\n",
    "pool = Pool(N_THREADS)\n",
    "data_chunks = np.array_split(np.array(list_of_strings), N_THREADS)\n",
    "#step 1:\n",
    "mapped = Parallel(n_jobs=N_THREADS)(delayed(chunks_mapper)(x) for x in data_chunks)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture is built using two functions: map and reduce . Each computation unit maps the input data and executes the initial reduce. Finally, some centralized unit executes the final reduce and returns the output. It looks like this:\n",
    "\n",
    "![](mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}